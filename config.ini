[JARS]
JUNIT_JAR = ./dependencies/junit-platform-console-standalone-1.9.2.jar
MOCKITO_JAR = ./dependencies/mockito-core-5.5.0.jar:./dependencies/mockito-junit-jupiter-3.9.0.jar:./dependencies/byte-buddy-1.14.4.jar:./dependencies/byte-buddy-agent-1.14.4.jar:

# Tokenizer settings
[MODEL]
# if a local model should be used to tokenize the input (run through llama-cpp-python)
# if set to false, the tiktoken tokenizer will be used
USE_MODEL = true
# path to a model which can be utilized by llama-cpp-python (ususally a .gguf file)
MODEL_PATH = vendor/model/mistral-7b-instruct-v0.1.Q5_K_M.gguf
MODEL_MAX_INPUT_TOKENS = 4096

[INFERENCE]
MODEL_MAX_OUTPUT_TOKENS = 2048
USE_HUGGINGFACE = true
HUGGINGFACE_INFERENCE_URL = https://api-inference.huggingface.co/models/codellama/CodeLlama-34b-Instruct-hf
# if a local webserver should be used to run the inference (run through llama-cpp-python web server module)
USE_LOCAL_WEB_SERVER = false
LOCAL_WEB_SERVER_PORT = 8000



